{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e11d8ee",
   "metadata": {},
   "source": [
    "# Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f34a9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.8\n",
      "IPython version      : 8.9.0\n",
      "\n",
      "torch         : 2.0.0.dev20230208\n",
      "wandb         : 0.13.10\n",
      "matplotlib    : 3.6.3\n",
      "seaborn       : 0.12.2\n",
      "numpy         : 1.24.1\n",
      "pandas        : 1.5.3\n",
      "albumentations: 1.3.0\n",
      "scipy         : 1.10.0\n",
      "sklearn       : 1.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\", font_scale=1.2)\n",
    "# rcParams[\"figure.figsize\"] = 16,10\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "import albumentations as A\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import scipy.io as io\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "import os\n",
    "from glob import glob\n",
    "import random\n",
    "import copy\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark -v -p torch,wandb,matplotlib,seaborn,numpy,pandas,albumentations,scipy,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7710a44",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c38ae",
   "metadata": {},
   "source": [
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96122b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int = 3, \n",
    "        channels: List[int] = [64, 128, 256, 512, 400],\n",
    "        kernel_sizes: List[int] = [11, 5, 5, 5, 8],\n",
    "        strides: List[int] = [4, 2, 2, 2, 1]\n",
    "    ) -> None:\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        layers = []\n",
    "        \n",
    "        for ix in range(len(channels)):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=in_channels, \n",
    "                        out_channels=channels[ix],\n",
    "                        kernel_size=kernel_sizes[ix], \n",
    "                        stride=strides[ix], \n",
    "                        padding=1\n",
    "                    ), \n",
    "                    nn.BatchNorm2d(channels[ix]),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            in_channels = channels[ix]\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "    def sample_normal(self, std: torch.Tensor) -> torch.Tensor:\n",
    "        sampler = torch.distributions.Normal(loc=0, scale=1)\n",
    "        return sampler.sample(std.shape)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        latent = self.net(x)\n",
    "        mu, std = latent[:, :200,], latent[:, 200:]\n",
    "        z = self.sample_normal(std)\n",
    "        latent = mu + z * std\n",
    "        return latent, mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64674bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv2d(512, 400, kernel_size=(8, 8), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = ImageEncoder()\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c55bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 62, 62]          23,296\n",
      "       BatchNorm2d-2           [-1, 64, 62, 62]             128\n",
      "              ReLU-3           [-1, 64, 62, 62]               0\n",
      "            Conv2d-4          [-1, 128, 30, 30]         204,928\n",
      "       BatchNorm2d-5          [-1, 128, 30, 30]             256\n",
      "              ReLU-6          [-1, 128, 30, 30]               0\n",
      "            Conv2d-7          [-1, 256, 14, 14]         819,456\n",
      "       BatchNorm2d-8          [-1, 256, 14, 14]             512\n",
      "              ReLU-9          [-1, 256, 14, 14]               0\n",
      "           Conv2d-10            [-1, 512, 6, 6]       3,277,312\n",
      "      BatchNorm2d-11            [-1, 512, 6, 6]           1,024\n",
      "             ReLU-12            [-1, 512, 6, 6]               0\n",
      "           Conv2d-13            [-1, 400, 1, 1]      13,107,600\n",
      "      BatchNorm2d-14            [-1, 400, 1, 1]             800\n",
      "             ReLU-15            [-1, 400, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 17,435,312\n",
      "Trainable params: 17,435,312\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.75\n",
      "Forward/backward pass size (MB): 9.85\n",
      "Params size (MB): 66.51\n",
      "Estimated Total Size (MB): 77.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(enc, input_size=(3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df859812",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dd14372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int = 200,\n",
    "        channels: List[int] = [512, 256, 128, 64, 1],\n",
    "        kernel_sizes: List[int] = [4, 4, 4, 4, 4],\n",
    "        strides: List[int] = [1, 2, 2, 2, 2], \n",
    "        paddings: List[int] = [0, 1, 1, 1, 1]\n",
    "    ) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for ix in range(len(channels)):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose3d(\n",
    "                        in_channels=in_channels, \n",
    "                        out_channels=channels[ix],\n",
    "                        stride=strides[ix],\n",
    "                        kernel_size=kernel_sizes[ix], \n",
    "                        padding=paddings[ix]\n",
    "                    ), \n",
    "                    nn.BatchNorm3d(channels[ix]), \n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            )\n",
    "            in_channels = channels[ix]\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce707cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ConvTranspose3d(200, 512, kernel_size=(4, 4, 4), stride=(1, 1, 1))\n",
       "      (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ConvTranspose3d(512, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ConvTranspose3d(256, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): ConvTranspose3d(128, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): ConvTranspose3d(64, 1, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "    )\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = Generator()\n",
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba9bea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose3d-1         [-1, 512, 4, 4, 4]       6,554,112\n",
      "       BatchNorm3d-2         [-1, 512, 4, 4, 4]           1,024\n",
      "              ReLU-3         [-1, 512, 4, 4, 4]               0\n",
      "   ConvTranspose3d-4         [-1, 256, 8, 8, 8]       8,388,864\n",
      "       BatchNorm3d-5         [-1, 256, 8, 8, 8]             512\n",
      "              ReLU-6         [-1, 256, 8, 8, 8]               0\n",
      "   ConvTranspose3d-7      [-1, 128, 16, 16, 16]       2,097,280\n",
      "       BatchNorm3d-8      [-1, 128, 16, 16, 16]             256\n",
      "              ReLU-9      [-1, 128, 16, 16, 16]               0\n",
      "  ConvTranspose3d-10       [-1, 64, 32, 32, 32]         524,352\n",
      "      BatchNorm3d-11       [-1, 64, 32, 32, 32]             128\n",
      "             ReLU-12       [-1, 64, 32, 32, 32]               0\n",
      "  ConvTranspose3d-13        [-1, 1, 64, 64, 64]           4,097\n",
      "      BatchNorm3d-14        [-1, 1, 64, 64, 64]               2\n",
      "             ReLU-15        [-1, 1, 64, 64, 64]               0\n",
      "          Sigmoid-16        [-1, 1, 64, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 17,570,627\n",
      "Trainable params: 17,570,627\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 71.75\n",
      "Params size (MB): 67.03\n",
      "Estimated Total Size (MB): 138.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(gen, input_size=(200, 1, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4dcaa",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51245e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int = 1, \n",
    "        channels: List[int] = [64, 128, 256, 512, 1],\n",
    "        kernel_sizes: List[int] = [4, 4, 4, 4, 4],\n",
    "        strides: List[int] = [4, 2, 2, 2, 1],\n",
    "    ) -> None:\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        for ix in range(len(channels)):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        in_channels=in_channels, \n",
    "                        out_channels=channels[ix], \n",
    "                        kernel_size=kernel_sizes[ix],\n",
    "                        stride=strides[ix], \n",
    "                        padding=1\n",
    "                    ), \n",
    "                    nn.BatchNorm3d(channels[ix]),\n",
    "                    nn.LeakyReLU(0.2)\n",
    "                )\n",
    "            )\n",
    "            in_channels = channels[ix]\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2c03857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (net): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv3d(1, 64, kernel_size=(4, 4, 4), stride=(4, 4, 4), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv3d(64, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): Conv3d(128, 256, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): Conv3d(256, 512, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): Conv3d(512, 1, kernel_size=(4, 4, 4), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "      (1): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disc = Discriminator()\n",
    "disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08837167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv3d-1       [-1, 64, 16, 16, 16]           4,160\n",
      "       BatchNorm3d-2       [-1, 64, 16, 16, 16]             128\n",
      "         LeakyReLU-3       [-1, 64, 16, 16, 16]               0\n",
      "            Conv3d-4         [-1, 128, 8, 8, 8]         524,416\n",
      "       BatchNorm3d-5         [-1, 128, 8, 8, 8]             256\n",
      "         LeakyReLU-6         [-1, 128, 8, 8, 8]               0\n",
      "            Conv3d-7         [-1, 256, 4, 4, 4]       2,097,408\n",
      "       BatchNorm3d-8         [-1, 256, 4, 4, 4]             512\n",
      "         LeakyReLU-9         [-1, 256, 4, 4, 4]               0\n",
      "           Conv3d-10         [-1, 512, 2, 2, 2]       8,389,120\n",
      "      BatchNorm3d-11         [-1, 512, 2, 2, 2]           1,024\n",
      "        LeakyReLU-12         [-1, 512, 2, 2, 2]               0\n",
      "           Conv3d-13           [-1, 1, 1, 1, 1]          32,769\n",
      "      BatchNorm3d-14           [-1, 1, 1, 1, 1]               2\n",
      "        LeakyReLU-15           [-1, 1, 1, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 11,049,795\n",
      "Trainable params: 11,049,795\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 7.97\n",
      "Params size (MB): 42.15\n",
      "Estimated Total Size (MB): 51.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(disc, input_size=(1, 64, 64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093b5de",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa49d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "with open('../../data/IKEA_imgs_shapes/list/bed.txt') as f:\n",
    "    files = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c5f07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_paths = []\n",
    "for file in files:\n",
    "    if file.strip() + '.mat' in os.listdir('../../data/IKEA_imgs_shapes/model/'):\n",
    "        output_paths.append(\n",
    "            os.path.join('../../data/IKEA_imgs_shapes/model/', f'{file.strip()}.mat')\n",
    "        )\n",
    "len(output_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f8a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    pixel_path: Optional[str] = None, \n",
    "    voxel_path: Optional[str] = None,\n",
    "):\n",
    "    pixel, voxel = None, None\n",
    "    if pixel_path:\n",
    "        pixel = plt.imread(pixel_path)\n",
    "    if voxel_path:\n",
    "        voxel = io.loadmat(voxel_path)\n",
    "    \n",
    "    return pixel, voxel['voxel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd57229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_paths = glob('../../data/IKEA_imgs_shapes/bed/*.png')\n",
    "pixel_paths = sorted(pixel_paths)\n",
    "len(pixel_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "837a4f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../../data/IKEA_imgs_shapes/bed/0001.png',\n",
       " '../../data/IKEA_imgs_shapes/model/IKEA_bed_MALM_malm_bed_2_obj0_object.mat')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_path, voxel_path = list(zip(pixel_paths, output_paths))[0]\n",
    "pixel_path, voxel_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30f6ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels, voxels = load_data(pixel_path, voxel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73e0f211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((244, 395, 3), (64, 64, 64))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixels.shape, voxels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7813f0f",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c2a62d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # thanks to awsaf for some amazing code style!!\n",
    "    seed = 101\n",
    "    exp_name = '2D_to_3D'\n",
    "    comment = 'Trying out the 2D to 3D GAN from 3dgan.csail'\n",
    "    model_name = 'VAE-3D-GAN'\n",
    "    train_bs = 4\n",
    "    valid_bs = 2*train_bs\n",
    "    image_size = [256, 256]\n",
    "    epochs = 5\n",
    "    scheduler = 'CosineAnnealingLR'\n",
    "    min_lr = 1e-6\n",
    "    T_max = int(30000/train_bs*epochs)+50\n",
    "    T_0 = 25\n",
    "    warmup_epochs = 0\n",
    "    n_accumulate  = max(1, 32//train_bs)\n",
    "    n_fold = 5\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_classes = None\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46f949",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2de689ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE3dGANDataset(Dataset):\n",
    "    def __init__(self, in_paths: List[str], out_paths: List[str], stage: str = \"train\") -> None:\n",
    "        super(VAE3dGANDataset, self).__init__()\n",
    "        self.in_paths = in_paths\n",
    "        self.out_paths = out_paths\n",
    "        self.stage = stage\n",
    "        \n",
    "        if self.stage == \"train\":\n",
    "            '''Ref: https://www.kaggle.com/code/alexeyolkhovikov/segformer-training'''\n",
    "            self.transforms = A.Compose([\n",
    "                A.augmentations.crops.RandomResizedCrop(height=cfg.image_size[0], width=cfg.image_size[1]),\n",
    "                A.augmentations.Rotate(limit=90, p=0.2),\n",
    "                A.augmentations.HorizontalFlip(p=0.2),\n",
    "                A.augmentations.VerticalFlip(p=0.2),\n",
    "                A.augmentations.transforms.ColorJitter(p=0.5),\n",
    "                A.OneOf([\n",
    "                    A.OpticalDistortion(p=0.2),\n",
    "                    A.GridDistortion(p=0.2),\n",
    "                    A.PiecewiseAffine(p=0.2)\n",
    "                ], p=0.5),\n",
    "                A.OneOf([\n",
    "                    A.HueSaturationValue(10, 15, 10),\n",
    "#                     A.CLAHE(clip_limit=4),\n",
    "                    A.RandomBrightnessContrast()\n",
    "                ], p=0.5),\n",
    "                A.Normalize()\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = A.Compose([\n",
    "                A.Resize(cfg.image_size[0], cfg.image_size[1]),\n",
    "                A.Normalize()\n",
    "            ])\n",
    "        \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.in_paths)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        in_path = self.in_paths[idx]\n",
    "        out_path = self.out_paths[idx]\n",
    "        \n",
    "        pixels, voxels = load_data(in_path, out_path)\n",
    "        \n",
    "        # the transform is applied to only the 2d image\n",
    "        pixels = self.transforms(image=pixels)\n",
    "        \n",
    "        return {\n",
    "            \"x\": torch.tensor(pixels['image'], dtype=torch.float32).permute(2, 0, 1).to(cfg.device),\n",
    "            \"y\": torch.tensor(voxels, dtype=torch.float32).unsqueeze(0).to(cfg.device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13f0c763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': torch.Size([3, 256, 256]), 'y': torch.Size([1, 64, 64, 64])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = VAE3dGANDataset(\n",
    "    in_paths=pixel_paths, \n",
    "    out_paths=output_paths, \n",
    "    stage=\"train\"\n",
    ")\n",
    "\n",
    "batch = dataset[0]\n",
    "{k:v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8cfd5429",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoader(dataset, batch_size=cfg.train_bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41175bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': torch.Size([4, 3, 256, 256]), 'y': torch.Size([4, 1, 64, 64, 64])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataset))\n",
    "{k:v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5dd35",
   "metadata": {},
   "source": [
    "### Optimizers and Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdb0fea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer):\n",
    "    '''Ref: https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-train-pytorch?scriptVersionId=97382768&cellId=55'''\n",
    "    if cfg.scheduler == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=optimizer, \n",
    "            T_max=cfg.T_max, \n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer=optimizer,\n",
    "            T_0=cfg.T_0, \n",
    "            eta_min=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ReduceLROnPlateau':\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=optimizer, \n",
    "            mode='min',\n",
    "            factor=0.1,\n",
    "            patience=0.7, \n",
    "            threshold=0.0001, \n",
    "            min_lr=cfg.min_lr\n",
    "        )\n",
    "    elif cfg.scheduler == 'ExponentialLR':\n",
    "        scheduler = lr_scheduler.ExponentialLR(\n",
    "            optimizer=optimizer, \n",
    "            gamma=0.85\n",
    "        )\n",
    "    elif cfg.scheduler == None:\n",
    "        return None\n",
    "    else:\n",
    "        raise NotImplementedError(f\"The scheduler `{cfg.scheduler}` has not been implememted\")\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab4219",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2f7cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SEEDING DONE\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    '''Ref: https://www.kaggle.com/code/awsaf49/uwmgi-2-5d-train-pytorch?scriptVersionId=97382768&cellId=15'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # When running on CudNN  backend, two further options\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    print('> SEEDING DONE')\n",
    "    \n",
    "set_seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33d2dcf",
   "metadata": {},
   "source": [
    "### Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6908786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data_pth: str = '../../data/IKEA_imgs_shapes.csv'):\n",
    "    df = pd.read_csv(data_pth)\n",
    "    df = df.sample(frac=1).reset_index()\n",
    "    df[\"kfold\"] = -1\n",
    "    kf = KFold(n_splits=cfg.n_fold)\n",
    "    \n",
    "    for fold, (train, valid) in enumerate(kf.split(df)):\n",
    "        df.loc[valid, \"kfold\"] = fold\n",
    "    \n",
    "    df.to_csv('../../data/IKEA_imgs_shapes_folds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1196e30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'y'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b36596c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 3, 256, 256]), torch.Size([4, 1, 64, 64, 64]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = batch['x'], batch['y']\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "02387202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 200, 1, 1]), torch.Size([4, 1, 64, 64, 64]), torch.Size([4]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = ImageEncoder()\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "a, mu, std = encoder.forward(x)\n",
    "b = generator.forward(a.unsqueeze(-1))\n",
    "c = discriminator.forward(b).squeeze()\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c6854e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[90.3263]], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kl_divergence(mu: torch.Tensor, std: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.mean(-0.5 * torch.sum(1 + std - mu ** 2 - std.exp(), dim = 1), dim = 0)\n",
    "\n",
    "def gfv_loss(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    return nn.MSELoss()(x, y)\n",
    "\n",
    "kl_div = kl_divergence(mu=mu, std=std)\n",
    "kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e59073f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ImageEncoder()\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "enc_optim = optim.Adam(encoder.parameters(), lr=3e-4)\n",
    "gen_optim = optim.Adam(generator.parameters(), lr=3e-4)\n",
    "dis_optim = optim.Adam(discriminator.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "068a7c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 1, 64, 64, 64]), torch.Size([4]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc05b552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:03<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10)):\n",
    "    a, mu, std = encoder.forward(x)\n",
    "    b = generator.forward(a.unsqueeze(-1))\n",
    "    c = discriminator.forward(b).squeeze()\n",
    "    \n",
    "    kl_loss = kl_divergence(mu, std)\n",
    "    gan_loss = nn.BCEWithLogitsLoss()(y, torch.ones_like(y)) + nn.BCEWithLogitsLoss()(c, torch.zeros_like(c))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (torch)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 08:08:27) [Clang 14.0.6 ]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
